{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b97d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import dspy\n",
    "\n",
    "\n",
    "with open(\"xai_key.txt\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=api_key)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "\n",
    "HW3_ROOT = os.getcwd()   \n",
    "PRAG_DATA_DIR = os.path.join(HW3_ROOT, \"PragmatiCQA\", \"data\")\n",
    "SOURCES_DIR   = os.path.join(HW3_ROOT, \"PragmatiCQA-sources\")\n",
    "\n",
    "VAL_JSONL   = os.path.join(PRAG_DATA_DIR, \"val.jsonl\")\n",
    "TRAIN_JSONL = os.path.join(PRAG_DATA_DIR, \"train.jsonl\")\n",
    "TEST_JSONL  = os.path.join(PRAG_DATA_DIR, \"test.jsonl\")\n",
    "\n",
    "\n",
    "\n",
    "TOP_K_RETRIEVE = 5 \n",
    "SEED = 42\n",
    "\n",
    "@dataclass\n",
    "class FirstTurnExample:\n",
    "    topic: str\n",
    "    question: str\n",
    "    gold_answer: str                  \n",
    "    literal_spans: List[str]           \n",
    "    pragmatic_spans: List[str]        \n",
    "    conversation_id: str               \n",
    "\n",
    "def read_jsonl(path: str) -> List[Dict]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbb6bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 179 first-question examples from val.jsonl\n",
      "Sample:\n",
      "topic: A Nightmare on Elm Street (2010 film)\n",
      "question: who is freddy krueger? ...\n",
      "gold_answer: Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wi ...\n",
      "#literal_spans: 1 | #pragmatic_spans: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Any\n",
    "\n",
    "def _extract_span_texts(objs: Optional[List[Dict[str, Any]]]) -> List[str]:\n",
    "    \"\"\"Safely extract the 'text' field from a_meta.literal_obj / pragmatic_obj entries.\"\"\"\n",
    "    if not objs:\n",
    "        return []\n",
    "    out = []\n",
    "    for obj in objs:\n",
    "        txt = obj.get(\"text\")\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            out.append(txt.strip())\n",
    "    return out\n",
    "\n",
    "def load_first_questions(val_jsonl_path: str) -> List[FirstTurnExample]:\n",
    "    rows = read_jsonl(val_jsonl_path)\n",
    "    examples: List[FirstTurnExample] = []\n",
    "\n",
    "    for i, conv in enumerate(rows):\n",
    "        topic = conv.get(\"topic\", \"\").strip()\n",
    "        qas = conv.get(\"qas\") or []\n",
    "        if not qas:\n",
    "            continue  \n",
    "\n",
    "        first = qas[0]\n",
    "        question = (first.get(\"q\") or \"\").strip()\n",
    "        gold_answer = (first.get(\"a\") or \"\").strip()\n",
    "\n",
    "        a_meta = first.get(\"a_meta\") or {}\n",
    "        literal_objs = a_meta.get(\"literal_obj\") or []\n",
    "        pragmatic_objs = a_meta.get(\"pragmatic_obj\") or []\n",
    "\n",
    "        literal_spans = _extract_span_texts(literal_objs)\n",
    "        pragmatic_spans = _extract_span_texts(pragmatic_objs)\n",
    "\n",
    "        ex = FirstTurnExample(\n",
    "            topic=topic or f\"[UNKNOWN_TOPIC_{i}]\",\n",
    "            question=question,\n",
    "            gold_answer=gold_answer,\n",
    "            literal_spans=literal_spans,\n",
    "            pragmatic_spans=pragmatic_spans,\n",
    "            conversation_id=str(i),\n",
    "        )\n",
    "        examples.append(ex)\n",
    "\n",
    "    return examples\n",
    "\n",
    "val_first_q_examples = load_first_questions(VAL_JSONL)\n",
    "print(f\"[INFO] Loaded {len(val_first_q_examples)} first-question examples from val.jsonl\")\n",
    "if val_first_q_examples:\n",
    "    e0 = val_first_q_examples[0]\n",
    "    print(\"Sample:\")\n",
    "    print(\"topic:\", e0.topic)\n",
    "    print(\"question:\", e0.question[:120], \"...\")\n",
    "    print(\"gold_answer:\", e0.gold_answer[:120], \"...\")\n",
    "    print(\"#literal_spans:\", len(e0.literal_spans), \"| #pragmatic_spans:\", len(e0.pragmatic_spans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07150a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: A Nightmare on Elm Street (2010 film)\n",
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n",
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Built FAISS index | topic='A Nightmare on Elm Street (2010 film)' | dir='A Nightmare on Elm Street' | #chunks=33827\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3 (Unified): Topic resolver + FAISS retriever ===\n",
    "from typing import List, Tuple, Optional\n",
    "import os, re, glob, difflib\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ----------------------------------------------------\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMB_MODEL_NAME)\n",
    "\n",
    "_topic_to_index: dict[str, faiss.Index] = {}\n",
    "_topic_to_chunks: dict[str, List[str]] = {}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "def normalize_topic_to_dirname(topic: str) -> str:\n",
    "    t = topic.strip()\n",
    "    t = re.sub(r\"\\s*\\([^)]*\\)\", \"\", t)          \n",
    "    t = re.sub(r\"[^0-9A-Za-z _-]+\", \" \", t)    \n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    t = t.replace(\" \", \"_\")\n",
    "    return t\n",
    "\n",
    "def list_available_topics_dirs() -> list:\n",
    "    if not os.path.isdir(SOURCES_DIR):\n",
    "        print(f\"[ERROR] Sources dir missing: {SOURCES_DIR}\")\n",
    "        return []\n",
    "    return [d for d in os.listdir(SOURCES_DIR) if os.path.isdir(os.path.join(SOURCES_DIR, d))]\n",
    "\n",
    "def resolve_topic_dir(topic: str) -> Optional[str]:\n",
    "    candidates = list_available_topics_dirs()\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    norm = normalize_topic_to_dirname(topic)\n",
    "    direct = os.path.join(SOURCES_DIR, norm)\n",
    "    if os.path.isdir(direct):\n",
    "        return direct\n",
    "\n",
    "    match = difflib.get_close_matches(norm, candidates, n=1, cutoff=0.6)\n",
    "    if match:\n",
    "        print(f\"[INFO] Fuzzy-resolved topic '{topic}' -> '{match[0]}'\")\n",
    "        return os.path.join(SOURCES_DIR, match[0])\n",
    "\n",
    "    raw = topic.replace(\" \", \"_\")\n",
    "    raw_match = difflib.get_close_matches(raw, candidates, n=1, cutoff=0.6)\n",
    "    if raw_match:\n",
    "        print(f\"[INFO] Fuzzy-resolved (raw) '{topic}' -> '{raw_match[0]}'\")\n",
    "        return os.path.join(SOURCES_DIR, raw_match[0])\n",
    "\n",
    "    print(f\"[WARN] Could not resolve topic folder for: {topic} (norm='{norm}')\")\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------------------\n",
    "def build_faiss_index_for_topic(topic: str) -> None:\n",
    "    topic_dir = resolve_topic_dir(topic)\n",
    "    if not topic_dir:\n",
    "        print(f\"[WARN] Topic dir not found for topic: {topic}\")\n",
    "        return\n",
    "\n",
    "    html_files = glob.glob(os.path.join(topic_dir, \"*.html\"))\n",
    "    chunks: List[str] = []\n",
    "    for f in html_files:\n",
    "        with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "            text = fh.read()\n",
    "        for para in text.split(\"\\n\"):\n",
    "            para = para.strip()\n",
    "            if len(para) > 30:\n",
    "                chunks.append(para)\n",
    "\n",
    "    if not chunks:\n",
    "        print(f\"[WARN] No chunks for topic '{topic}' (dir='{topic_dir}')\")\n",
    "        return\n",
    "\n",
    "    X = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n",
    "    faiss.normalize_L2(X)\n",
    "    index = faiss.IndexFlatIP(X.shape[1]) \n",
    "    index.add(X)\n",
    "\n",
    "    _topic_to_index[topic] = index\n",
    "    _topic_to_chunks[topic] = chunks\n",
    "    print(f\"[INFO] Built FAISS index | topic='{topic}' | dir='{os.path.basename(topic_dir)}' | #chunks={len(chunks)}\")\n",
    "\n",
    "def retrieve_context(topic: str, question: str, top_k: int = TOP_K_RETRIEVE) -> List[str]:\n",
    "    resolved_dir = resolve_topic_dir(topic)\n",
    "    if not resolved_dir:\n",
    "        print(f\"[WARN] Could not resolve dir for topic: {topic}\")\n",
    "\n",
    "    if topic not in _topic_to_index:\n",
    "        build_faiss_index_for_topic(topic)\n",
    "    if topic not in _topic_to_index:\n",
    "        return []\n",
    "\n",
    "    index = _topic_to_index[topic]\n",
    "    chunks = _topic_to_chunks[topic]\n",
    "\n",
    "    q_emb = embedder.encode([question], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "\n",
    "    return [chunks[i] for i in I[0] if 0 <= i < len(chunks)]\n",
    "\n",
    "sample_ex = val_first_q_examples[0]\n",
    "print(\"Topic:\", sample_ex.topic)\n",
    "ctx = retrieve_context(sample_ex.topic, sample_ex.question, top_k=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef5da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n",
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n",
      "Coop Query: Detailed biography of Freddy Krueger: origins in A Nightmare on Elm Street, creator, characteristics, and cultural significance.\n",
      "Answer (preview): Freddy Krueger, whose full name is Frederick Charles \"Freddy\" Krueger, is a fictional character from the \"A Nightmare on Elm Street\" horror film franchise. He is often referred to as Fred Krueger and is portrayed as a vengeful dream-haunting antagonist with a burned appearance and a signature clawed ...\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from typing import List, Tuple, Optional\n",
    "class SummarizeGoal(dspy.Signature):\n",
    "    \"\"\"Summarize the student's long-term goal or interests from prior Q/A history.\"\"\"\n",
    "    history = dspy.InputField(desc=\"List of prior (question, answer) pairs, oldest→newest.\")\n",
    "    summary = dspy.OutputField(desc=\"2-4 concise sentences summarizing goals/interests.\")\n",
    "\n",
    "class InferPragmaticNeed(dspy.Signature):\n",
    "    \"\"\"Infer the current pragmatic need behind the student's question.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    history_summary = dspy.InputField()\n",
    "    retrieved_glimpse = dspy.InputField(desc=\"Short excerpt(s) of the retrieved context.\")\n",
    "    need = dspy.OutputField(desc=\"Crisp statement of what extra info would be most helpful now.\")\n",
    "\n",
    "class GenerateCoopQuery(dspy.Signature):\n",
    "    \"\"\"Generate a cooperative follow-up retrieval query to fetch complementary context.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    pragmatic_need = dspy.InputField()\n",
    "    coop_query = dspy.OutputField(desc=\"A single focused query for the retriever (<= 160 chars).\")\n",
    "\n",
    "class ReasonCoT(dspy.Signature):\n",
    "    \"\"\"Deliberate on how to craft a cooperative answer grounded in the retrieved evidence.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    history_summary = dspy.InputField()\n",
    "    all_context = dspy.InputField(desc=\"Concise merged evidence snippets to ground the answer.\")\n",
    "    pragmatic_need = dspy.InputField()\n",
    "    reasoning = dspy.OutputField(desc=\"Step-by-step plan: literal facts to include + helpful extras.\")\n",
    "\n",
    "class CooperativeAnswer(dspy.Signature):\n",
    "    \"\"\"Produce a cooperative answer grounded in retrieved evidence.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    history_summary = dspy.InputField()\n",
    "    all_context = dspy.InputField()\n",
    "    reasoning = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Final cooperative answer. Cite evidence implicitly; avoid hallucinations; be concise and helpful.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "def _format_history(history: Optional[List[Tuple[str, str]]]) -> str:\n",
    "    if not history:\n",
    "        return \"(no prior turns)\"\n",
    "    lines = []\n",
    "    for i, (q, a) in enumerate(history, 1):\n",
    "        lines.append(f\"Turn {i} - Q: {q}\\nTurn {i} - A: {a}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _shorten_chunks(chunks: List[str], max_chars: int = 1000) -> str:\n",
    "    \"\"\"Concatenate chunks with a soft character budget (for prompting).\"\"\"\n",
    "    out, used = [], 0\n",
    "    for c in chunks:\n",
    "        c = c.strip()\n",
    "        if not c:\n",
    "            continue\n",
    "        if used + len(c) + 2 > max_chars:\n",
    "            break\n",
    "        out.append(c)\n",
    "        used += len(c) + 2\n",
    "    return \"\\n---\\n\".join(out)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "class MultiStepCoopQAModule(dspy.Module):\n",
    "    def __init__(self, retriever_fn, top_k: int = 5, second_hop: bool = True):\n",
    "        \"\"\"\n",
    "        retriever_fn: callable(topic: str, question: str, top_k: int) -> List[str]\n",
    "        top_k: כמה קטעים לשלוף בכל שאילתה.\n",
    "        second_hop: האם לבצע שליפה שניה עם ה-coop query.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.retriever_fn = retriever_fn\n",
    "        self.top_k = top_k\n",
    "        self.second_hop = second_hop\n",
    "\n",
    "        self.summarize = dspy.Predict(SummarizeGoal)\n",
    "        self.infer_need = dspy.Predict(InferPragmaticNeed)\n",
    "        self.gen_query = dspy.Predict(GenerateCoopQuery)\n",
    "        self.reason = dspy.Predict(ReasonCoT)\n",
    "        self.answer = dspy.Predict(CooperativeAnswer)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        topic: str,\n",
    "        question: str,\n",
    "        history: Optional[List[Tuple[str, str]]] = None,\n",
    "        initial_context: Optional[List[str]] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        API ראשי:\n",
    "          - topic: שם הטופיק (לשליפה מתוך ה-HTMLים שלו)\n",
    "          - question: השאלה הנוכחית\n",
    "          - history: [(q,a), ...] (ב-4.4.1 לרוב None)\n",
    "          - initial_context: אם כבר שלפת קטעים מראש (אפשר להשאיר None)\n",
    "          - top_k: גובר על self.top_k אם סופק\n",
    "        מחזיר: dict עם שלבי ביניים + answer סופי.\n",
    "        \"\"\"\n",
    "        k = top_k or self.top_k\n",
    "\n",
    "        ctx1 = initial_context\n",
    "        if ctx1 is None:\n",
    "            ctx1 = self.retriever_fn(topic, question, top_k=k)\n",
    "        ctx1_short = _shorten_chunks(ctx1, max_chars=800) if ctx1 else \"(no context retrieved)\"\n",
    "\n",
    "        history_txt = _format_history(history)\n",
    "\n",
    "        summary = self.summarize(history=history_txt).summary\n",
    "        need = self.infer_need(\n",
    "            question=question,\n",
    "            history_summary=summary,\n",
    "            retrieved_glimpse=ctx1_short\n",
    "        ).need\n",
    "\n",
    "        coop_q = self.gen_query(\n",
    "            question=question,\n",
    "            pragmatic_need=need\n",
    "        ).coop_query\n",
    "\n",
    "        ctx_all = list(ctx1) if ctx1 else []\n",
    "        if self.second_hop and coop_q and isinstance(coop_q, str) and len(coop_q.strip()) > 0:\n",
    "            ctx2 = self.retriever_fn(topic, coop_q.strip(), top_k=max(2, k // 2))\n",
    "            if ctx2:\n",
    "                ctx_all.extend(ctx2)\n",
    "\n",
    "        ctx_all_short = _shorten_chunks(ctx_all, max_chars=1600) if ctx_all else ctx1_short\n",
    "        plan = self.reason(\n",
    "            question=question,\n",
    "            history_summary=summary,\n",
    "            all_context=ctx_all_short,\n",
    "            pragmatic_need=need\n",
    "        ).reasoning\n",
    "\n",
    "        final = self.answer(\n",
    "            question=question,\n",
    "            history_summary=summary,\n",
    "            all_context=ctx_all_short,\n",
    "            reasoning=plan\n",
    "        ).answer\n",
    "\n",
    "        return {\n",
    "            \"history_summary\": summary,\n",
    "            \"pragmatic_need\": need,\n",
    "            \"coop_query\": coop_q,\n",
    "            \"context_1\": ctx1[:k] if ctx1 else [],\n",
    "            \"context_all\": ctx_all[: (k + max(2, k // 2)) ],\n",
    "            \"reasoning\": plan,\n",
    "            \"answer\": final,\n",
    "        }\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "multi_step = MultiStepCoopQAModule(retriever_fn=retrieve_context, top_k=TOP_K_RETRIEVE, second_hop=True)\n",
    "\n",
    "probe = multi_step(\n",
    "    topic=val_first_q_examples[0].topic,\n",
    "    question=val_first_q_examples[0].question,\n",
    "    history=None,               \n",
    "    initial_context=None,       \n",
    "    top_k=3\n",
    ")\n",
    "print(\"Coop Query:\", probe[\"coop_query\"])\n",
    "print(\"Answer (preview):\", (probe[\"answer\"] or \"\")[:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc512989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n",
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   1%|          | 1/179 [00:26<1:19:09, 26.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n",
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   1%|          | 2/179 [00:49<1:12:44, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n",
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   2%|▏         | 3/179 [01:16<1:14:41, 25.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n",
      "[INFO] Fuzzy-resolved topic 'A Nightmare on Elm Street (2010 film)' -> 'A Nightmare on Elm Street'\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\A Nightmare on Elm Street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   2%|▏         | 4/179 [01:55<1:29:32, 30.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Built FAISS index | topic='Batman' | dir='Batman' | #chunks=122677\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   3%|▎         | 5/179 [33:20<33:48:19, 699.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   3%|▎         | 6/179 [33:48<22:38:25, 471.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   4%|▍         | 7/179 [34:11<15:31:09, 324.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   4%|▍         | 8/179 [34:35<10:52:10, 228.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   5%|▌         | 9/179 [35:03<7:51:05, 166.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   6%|▌         | 10/179 [35:31<5:47:54, 123.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   6%|▌         | 11/179 [35:57<4:22:30, 93.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   7%|▋         | 12/179 [36:24<3:23:48, 73.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   7%|▋         | 13/179 [36:59<2:50:50, 61.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   8%|▊         | 14/179 [37:23<2:18:29, 50.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:   9%|▉         | 17/179 [37:52<1:03:54, 23.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  10%|█         | 18/179 [38:17<1:04:08, 23.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  11%|█         | 19/179 [38:43<1:05:25, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  11%|█         | 20/179 [39:06<1:03:31, 23.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  12%|█▏        | 21/179 [39:37<1:08:29, 26.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  12%|█▏        | 22/179 [40:03<1:08:00, 25.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  13%|█▎        | 23/179 [40:31<1:09:18, 26.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  13%|█▎        | 24/179 [41:01<1:11:24, 27.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  14%|█▍        | 25/179 [41:34<1:14:35, 29.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  15%|█▍        | 26/179 [42:11<1:19:56, 31.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  15%|█▌        | 27/179 [42:42<1:19:19, 31.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  16%|█▌        | 28/179 [43:11<1:16:58, 30.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  16%|█▌        | 29/179 [43:38<1:13:46, 29.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  17%|█▋        | 30/179 [44:05<1:11:39, 28.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  17%|█▋        | 31/179 [44:33<1:10:21, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  18%|█▊        | 32/179 [44:59<1:08:12, 27.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  18%|█▊        | 33/179 [45:33<1:12:21, 29.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  19%|█▉        | 34/179 [45:58<1:08:16, 28.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  20%|█▉        | 35/179 [46:20<1:03:16, 26.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  20%|██        | 36/179 [46:45<1:01:43, 25.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  21%|██        | 37/179 [47:23<1:10:21, 29.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n",
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultiStep on first questions:  21%|██        | 38/179 [47:50<1:08:01, 28.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using topic dir: c:\\Users\\gilic\\hw3\\nlp-with-llms-2025-hw3\\PragmatiCQA-sources\\Batman\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Run Multi-Step module on first questions and save outputs ===\n",
    "import os, json, time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "OUT_DIR = os.path.join(HW3_ROOT, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "PRED_JSONL = os.path.join(OUT_DIR, \"val_firstq_multistep_predictions.jsonl\")\n",
    "\n",
    "def run_first_questions_multistep(\n",
    "    examples: List[FirstTurnExample],\n",
    "    module: MultiStepCoopQAModule,\n",
    "    top_k: int = TOP_K_RETRIEVE,\n",
    "    limit: Optional[int] = None,\n",
    "    save_path: Optional[str] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    N = len(examples) if limit is None else min(limit, len(examples))\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ex in tqdm(examples[:N], total=N, desc=\"MultiStep on first questions\"):\n",
    "        try:\n",
    "            initial_ctx = retrieve_context(ex.topic, ex.question, top_k=top_k)\n",
    "\n",
    "            out = module(\n",
    "                topic=ex.topic,\n",
    "                question=ex.question,\n",
    "                history=None,             \n",
    "                initial_context=initial_ctx,\n",
    "                top_k=top_k\n",
    "            )\n",
    "\n",
    "            rec: Dict[str, Any] = {\n",
    "                \"conversation_id\": ex.conversation_id,\n",
    "                \"topic\": ex.topic,\n",
    "                \"question\": ex.question,\n",
    "                \"pred_answer\": out.get(\"answer\", \"\"),\n",
    "                \"gold_answer\": ex.gold_answer,\n",
    "                \"literal_spans\": ex.literal_spans,\n",
    "                \"pragmatic_spans\": ex.pragmatic_spans,\n",
    "                \"history_summary\": out.get(\"history_summary\", \"\"),\n",
    "                \"pragmatic_need\": out.get(\"pragmatic_need\", \"\"),\n",
    "                \"coop_query\": out.get(\"coop_query\", \"\"),\n",
    "                \"context_1\": out.get(\"context_1\", []),\n",
    "                \"context_all\": out.get(\"context_all\", []),\n",
    "                \"reasoning\": out.get(\"reasoning\", \"\"),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            rec = {\n",
    "                \"conversation_id\": ex.conversation_id,\n",
    "                \"topic\": ex.topic,\n",
    "                \"question\": ex.question,\n",
    "                \"pred_answer\": \"\",\n",
    "                \"gold_answer\": ex.gold_answer,\n",
    "                \"literal_spans\": ex.literal_spans,\n",
    "                \"pragmatic_spans\": ex.pragmatic_spans,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "        results.append(rec)\n",
    "\n",
    "        if save_path:\n",
    "            with open(save_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[INFO] Done {N} examples in {dt:.1f}s ({dt/max(N,1):.2f}s/ex)\")\n",
    "    print(f\"[INFO] Saved to: {save_path}\" if save_path else \"[INFO] Not saved (save_path=None)\")\n",
    "    return results\n",
    "\n",
    "if os.path.exists(PRED_JSONL):\n",
    "    os.remove(PRED_JSONL)\n",
    "\n",
    "val_firstq_preds = run_first_questions_multistep(\n",
    "    val_first_q_examples,\n",
    "    multi_step,\n",
    "    top_k=TOP_K_RETRIEVE,\n",
    "    limit=None,                   \n",
    "    save_path=PRED_JSONL\n",
    ")\n",
    "\n",
    "print(f\"Total predictions: {len(val_firstq_preds)}\")\n",
    "for rec in val_firstq_preds[:2]:\n",
    "    print(\"-\"*80)\n",
    "    print(\"Q:\", rec[\"question\"])\n",
    "    print(\"Pred:\", (rec[\"pred_answer\"] or \"\")[:300], \"...\")\n",
    "    print(\"Gold:\", (rec[\"gold_answer\"] or \"\")[:300], \"...\")\n",
    "    print(\"Coop Query:\", rec.get(\"coop_query\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from statistics import mean\n",
    "PRED_JSONL = os.path.join(HW3_ROOT, \"outputs\", \"val_firstq_multistep_predictions.jsonl\")\n",
    "if 'val_firstq_preds' not in globals() or not isinstance(val_firstq_preds, list) or len(val_firstq_preds) == 0:\n",
    "    val_firstq_preds = []\n",
    "    if os.path.exists(PRED_JSONL):\n",
    "        with open(PRED_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    val_firstq_preds.append(json.loads(line))\n",
    "        print(f\"[INFO] Loaded {len(val_firstq_preds)} predictions from file.\")\n",
    "    else:\n",
    "        raise RuntimeError(\"[ERROR] No predictions found in memory or on disk.\")\n",
    "\n",
    "\n",
    "EVAL_LIMIT = None \n",
    "\n",
    "pairs = []\n",
    "for rec in val_firstq_preds[: (EVAL_LIMIT or len(val_firstq_preds))]:\n",
    "    pred = (rec.get(\"pred_answer\") or \"\").strip()\n",
    "    gold = (rec.get(\"gold_answer\") or \"\").strip()\n",
    "    if pred == \"\" and gold == \"\":\n",
    "        continue\n",
    "    pairs.append((pred, gold))\n",
    "\n",
    "print(f\"[INFO] Evaluating {len(pairs)} pairs with SemanticF1...\")\n",
    "\n",
    "try:\n",
    "    from dspy.evaluate import SemanticF1\n",
    "except Exception:\n",
    "    try:\n",
    "        from dspy.evaluation import SemanticF1\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            f\"Could not import SemanticF1 from dspy. \"\n",
    "            f\"Please ensure DSPy is up to date. Original error: {e}\"\n",
    "        )\n",
    "\n",
    "\n",
    "metric = SemanticF1()\n",
    "precisions, recalls, f1s = [], [], []\n",
    "try:\n",
    "    preds = [p for p, _ in pairs]\n",
    "    gts   = [g for _, g in pairs]\n",
    "    batch_scores = metric.batch(predictions=preds, references=gts)\n",
    "    for s in batch_scores:\n",
    "        precisions.append(float(s.get('precision', 0.0)))\n",
    "        recalls.append(float(s.get('recall', 0.0)))\n",
    "        f1s.append(float(s.get('f1', 0.0)))\n",
    "except Exception:\n",
    "    for pred, gold in pairs:\n",
    "        s = metric(prediction=pred, reference=gold)\n",
    "        precisions.append(float(s.get('precision', 0.0)))\n",
    "        recalls.append(float(s.get('recall', 0.0)))\n",
    "        f1s.append(float(s.get('f1', 0.0)))\n",
    "\n",
    "def _avg(x): \n",
    "    return round(mean(x), 4) if x else 0.0\n",
    "\n",
    "report = {\n",
    "    \"N\": len(pairs),\n",
    "    \"precision_avg\": _avg(precisions),\n",
    "    \"recall_avg\": _avg(recalls),\n",
    "    \"f1_avg\": _avg(f1s),\n",
    "}\n",
    "\n",
    "print(\"\\n=== SemanticF1 Report (Multi-Step, first questions) ===\")\n",
    "for k, v in report.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "METRICS_JSON = os.path.join(HW3_ROOT, \"outputs\", \"val_firstq_multistep_metrics.json\")\n",
    "with open(METRICS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[INFO] Metrics saved to: {METRICS_JSON}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
